{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from collections import OrderedDict\n",
    "import logging\n",
    "import os\n",
    "import pickle\n",
    "import pathlib\n",
    "temp = pathlib.PosixPath\n",
    "pathlib.PosixPath = pathlib.WindowsPath\n",
    "\n",
    "import torch\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from datasets import Dataset, load_dataset\n",
    "\n",
    "from model.model import SentenceTransformersWrapperForLM"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\anaconda3\\envs\\dl-env\\lib\\site-packages\\torch\\nn\\modules\\lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
      "  warnings.warn('Lazy modules are a new feature under heavy development '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from E:\\OneDrive - Hanoi University of Science and Technology\\Chuyen nganh\\Deep Learning and Its Applications\\BTL\\save\\models\\1211_113344\\checkpoint-epoch1.pth\n"
     ]
    }
   ],
   "source": [
    "# extrac encoder from SentenceTransformersWrapperForLM\n",
    "model_load_path = 'E:\\\\OneDrive - Hanoi University of Science and Technology\\\\Chuyen nganh\\\\Deep Learning and Its Applications\\\\BTL\\\\save\\\\models\\\\1211_113344\\\\checkpoint-epoch1.pth'\n",
    "stf_wrapper = SentenceTransformersWrapperForLM(\n",
    "    model_name='all-MiniLM-L6-v2',\n",
    "    model_path=None,\n",
    "    hidden_size=512,\n",
    "    dropout=0.1,\n",
    "    vocab_size=32000,\n",
    "    load_path=model_load_path\n",
    ")\n",
    "sentence_transformers_model = stf_wrapper.model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_save_path = 'E:\\\\OneDrive - Hanoi University of Science and Technology\\\\Chuyen nganh\\\\Deep Learning and Its Applications\\\\BTL\\\\save\\\\models\\\\1211_113344\\\\sentence_transformers_model.pth'\n",
    "torch.save(sentence_transformers_model.state_dict(), model_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "COLLECTION_PATH = 'E:\\\\OneDrive - Hanoi University of Science and Technology\\\\Chuyen nganh\\\\Deep Learning and Its Applications\\\\BTL\\\\data\\\\MSMACRO\\\\passage_ranking_dataset\\\\collection.tsv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BERTBiEncoder(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        query_encoder_name,\n",
    "        query_encoder_path,\n",
    "        passage_encoder_name,\n",
    "        passage_encoder_path,\n",
    "        vocab_size,\n",
    "        load_path=None,\n",
    "        query_encoder_load_path=None,\n",
    "        passage_encoder_load_path=None,\n",
    "    ):\n",
    "        super(BERTBiEncoder, self).__init__()\n",
    "        \n",
    "        self.query_encoder_name = query_encoder_name\n",
    "        self.query_encoder_path = query_encoder_path\n",
    "        self.passage_encoder_name = passage_encoder_name\n",
    "        self.passage_encoder_path = passage_encoder_path\n",
    "        self.vocab_size = vocab_size\n",
    "\n",
    "        if query_encoder_name:\n",
    "            self.query_encoder = SentenceTransformer(query_encoder_name)\n",
    "        elif query_encoder_path:\n",
    "            self.query_encoder = SentenceTransformer(query_encoder_path)\n",
    "        self._extend_encoder_vocab_size(encoder=self.query_encoder, new_vocab_size=vocab_size)\n",
    "        \n",
    "        if passage_encoder_name:\n",
    "            self.passage_encoder = SentenceTransformer(passage_encoder_name)\n",
    "        elif passage_encoder_path:\n",
    "            self.passage_encoder = SentenceTransformer(passage_encoder_path)\n",
    "        self._extend_encoder_vocab_size(encoder=self.passage_encoder, new_vocab_size=vocab_size)\n",
    "        \n",
    "        if load_path is not None:\n",
    "            torch_model_checkpoint = torch.load(load_path, map_location='cpu')\n",
    "            self.load_state_dict(torch_model_checkpoint['state_dict'])\n",
    "            print('Loaded model from', load_path)\n",
    "        if query_encoder_load_path is not None:\n",
    "            torch_model_checkpoint = torch.load(query_encoder_load_path, map_location='cpu')\n",
    "            self.query_encoder.load_state_dict(torch_model_checkpoint['state_dict'])\n",
    "            print('Loaded query encoder from', query_encoder_load_path)\n",
    "        if passage_encoder_load_path is not None:\n",
    "            torch_model_checkpoint = torch.load(passage_encoder_load_path, map_location='cpu')\n",
    "            self.passage_encoder.load_state_dict(torch_model_checkpoint['state_dict'])\n",
    "            print('Loaded model from', passage_encoder_load_path)\n",
    "\n",
    "    def forward(self, query_features, passage_features):\n",
    "        query_embeddings = self.query_encoder.forward(query_features)['sentence_embedding']\n",
    "        passage_embeddings = self.passage_encoder.forward(passage_features)['sentence_embedding']\n",
    "        scores = torch.stack([F.cosine_similarity(q_emb, passage_embeddings) for q_emb in query_embeddings])\n",
    "        outputs = {\n",
    "            'scores': scores,\n",
    "            'query_embeddings': query_embeddings,\n",
    "            'passage_embeddings': passage_embeddings\n",
    "        }\n",
    "        return outputs\n",
    "\n",
    "    def _extend_encoder_vocab_size(self, encoder: SentenceTransformer, new_vocab_size=None):\n",
    "        '''\n",
    "        Update model embedding layer for larger vocabulary, keeps all the trained embeddings\n",
    "        '''\n",
    "        if not new_vocab_size:\n",
    "            new_vocab_size = self.vocab_size\n",
    "        old_embedding_layer = encoder._first_module().auto_model.embeddings.word_embeddings\n",
    "        old_vocab_size = old_embedding_layer.weight.shape[0]\n",
    "        embedding_size = old_embedding_layer.weight.shape[1]\n",
    "        new_embedding_layer = nn.Embedding(num_embeddings=new_vocab_size, embedding_dim=embedding_size)\n",
    "        new_embedding_layer.weight.data[:old_vocab_size] = old_embedding_layer.weight.data\n",
    "        encoder._first_module().auto_model.embeddings.word_embeddings = new_embedding_layer        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_specific_line_of_file(file_path, line_number):\n",
    "    with open(file_path, 'r') as f:\n",
    "        for i, line in enumerate(f):\n",
    "            if i == line_number:\n",
    "                return line.strip()\n",
    "    return None        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def constrastive_loss(scores, tau=1):\n",
    "    '''\n",
    "    Loss for constrastive learning\n",
    "    Score for positive sample always at first\n",
    "    '''\n",
    "    exp_scores = torch.exp(scores/tau)\n",
    "    loss = -torch.log(exp_scores[0]/exp_scores.sum())\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "input_features = {\n",
    "    'input_ids': torch.tensor([[1,2,3,4], [2,3,4,5]]),\n",
    "    'attention_mask': torch.tensor([[1,1,1,1], [1,1,1,1]]),\n",
    "    'token_type_ids': torch.tensor([[0,0,0,0], [0,0,0,0]])\n",
    "}\n",
    "out_features = sentence_transformers_model.forward(input_features)\n",
    "embeddings = []\n",
    "for sent_idx in range(len(out_features['sentence_embedding'])):\n",
    "    row = {name: out_features[name][sent_idx] for name in out_features}\n",
    "    embeddings.append(row)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.7553, 0.7285],\n",
       "        [0.8283, 0.9175]], grad_fn=<StackBackward0>)"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "biencoder = BERTBiEncoder(\n",
    "    query_encoder_name='all-MiniLM-L6-v2',\n",
    "    query_encoder_path=None,\n",
    "    passage_encoder_name='all-MiniLM-L6-v2',\n",
    "    passage_encoder_path=None,\n",
    "    vocab_size=32000,\n",
    "    load_path=None,\n",
    "    query_encoder_load_path=None,\n",
    "    passage_encoder_load_path=None,\n",
    ")\n",
    "q_input_features = {\n",
    "    'input_ids': torch.tensor([[1,3200,3,4], [2,3,11,5]]),\n",
    "    'attention_mask': torch.tensor([[1,1,1,1], [1,1,1,1]]),\n",
    "    'token_type_ids': torch.tensor([[0,0,0,0], [0,0,0,0]])\n",
    "}\n",
    "p_input_features = {\n",
    "    'input_ids': torch.tensor([[1,0,3,4], [2,1,4,5]]),\n",
    "    'attention_mask': torch.tensor([[1,1,1,1], [1,1,1,1]]),\n",
    "    'token_type_ids': torch.tensor([[0,0,0,0], [0,0,0,0]])\n",
    "}\n",
    "output = biencoder.forward(q_input_features, p_input_features)\n",
    "output['scores']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 384])"
      ]
     },
     "execution_count": 46,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output['query_embeddings'].shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([2, 384])"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output['passage_embeddings']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "ename": "RuntimeError",
     "evalue": "The size of tensor a (5) must match the size of tensor b (4) at non-singleton dimension 1",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[59], line 3\u001b[0m\n\u001b[0;32m      1\u001b[0m q \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([[\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m3\u001b[39m,\u001b[38;5;241m4\u001b[39m], [\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m3\u001b[39m,\u001b[38;5;241m11\u001b[39m,\u001b[38;5;241m5\u001b[39m]], dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[0;32m      2\u001b[0m p \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([[\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m3\u001b[39m,\u001b[38;5;241m4\u001b[39m], [\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m4\u001b[39m,\u001b[38;5;241m5\u001b[39m]], dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m----> 3\u001b[0m torch\u001b[38;5;241m.\u001b[39mstack([F\u001b[38;5;241m.\u001b[39mcosine_similarity(q_emb, p) \u001b[38;5;28;01mfor\u001b[39;00m q_emb \u001b[38;5;129;01min\u001b[39;00m q])\n",
      "Cell \u001b[1;32mIn[59], line 3\u001b[0m, in \u001b[0;36m<listcomp>\u001b[1;34m(.0)\u001b[0m\n\u001b[0;32m      1\u001b[0m q \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([[\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m3\u001b[39m,\u001b[38;5;241m4\u001b[39m], [\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m3\u001b[39m, \u001b[38;5;241m3\u001b[39m,\u001b[38;5;241m11\u001b[39m,\u001b[38;5;241m5\u001b[39m]], dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[0;32m      2\u001b[0m p \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mtensor([[\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m0\u001b[39m,\u001b[38;5;241m3\u001b[39m,\u001b[38;5;241m4\u001b[39m], [\u001b[38;5;241m2\u001b[39m,\u001b[38;5;241m1\u001b[39m,\u001b[38;5;241m4\u001b[39m,\u001b[38;5;241m5\u001b[39m]], dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat32)\n\u001b[1;32m----> 3\u001b[0m torch\u001b[38;5;241m.\u001b[39mstack([\u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcosine_similarity\u001b[49m\u001b[43m(\u001b[49m\u001b[43mq_emb\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mp\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mfor\u001b[39;00m q_emb \u001b[38;5;129;01min\u001b[39;00m q])\n",
      "\u001b[1;31mRuntimeError\u001b[0m: The size of tensor a (5) must match the size of tensor b (4) at non-singleton dimension 1"
     ]
    }
   ],
   "source": [
    "q = torch.tensor([[1,2,3, 3,4], [2,3, 3,11,5]], dtype=torch.float32)\n",
    "p = torch.tensor([[1,0,3,4], [2,1,4,5]], dtype=torch.float32)\n",
    "torch.stack([F.cosine_similarity(q_emb, p) for q_emb in q])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('E:\\\\OneDrive - Hanoi University of Science and Technology\\\\Chuyen nganh\\\\Deep Learning and Its Applications\\\\BTL\\\\data\\\\MSMACRO\\\\passage_ranking_dataset\\\\train_qrels_first_100k_with_negatives.pkl', 'rb') as f:\n",
    "    train_qrels = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class MSMarcoConstrastiveLearningDataset(Dataset):\n",
    "    '''\n",
    "    MS Marco dataset for constrastive learning\n",
    "    '''\n",
    "\n",
    "    def __init__(\n",
    "        self,\n",
    "        qrels_path,\n",
    "    )\n",
    "    with open(qrels_path, 'rb') as f:\n",
    "        self.qrels = pickle.load(f) \n",
    "\n",
    "    def __len__(self) -> int:\n",
    "        return len(self.dataset)\n",
    "\n",
    "    def __getitem__(self, index: int):\n",
    "        # Tokenize the data and perform other preprocessing\n",
    "        return self.dataset[]"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
