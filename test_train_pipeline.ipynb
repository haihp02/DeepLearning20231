{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "from collections import OrderedDict\n",
    "import logging\n",
    "import os\n",
    "import pathlib\n",
    "temp = pathlib.PosixPath\n",
    "pathlib.PosixPath = pathlib.WindowsPath\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "from transformers import AutoTokenizer\n",
    "from sentence_transformers import SentenceTransformer\n",
    "from sentence_transformers.models import Transformer\n",
    "\n",
    "from parse_config import ConfigParser\n",
    "import data_loader.data_loaders as module_data\n",
    "import model.model as module_arch\n",
    "import model.metric as module_metric\n",
    "import model.loss as module_loss\n",
    "from trainer.trainer import Trainer, BertTrainer\n",
    "from utils import prepare_device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fix random seeds for reproducibility\n",
    "SEED = 123\n",
    "torch.manual_seed(SEED)\n",
    "torch.backends.cudnn.deterministic = True\n",
    "torch.backends.cudnn.benchmark = False\n",
    "np.random.seed(SEED)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "config = {\n",
    "    \"name\": \"sentence_bert\",\n",
    "    \"n_gpu\": 1,\n",
    "    \"tokenizer\": {\n",
    "        \"path\": \"./saved/tokenizer/tokenizer-all-MiniLM-L6-v2-msmarco\"\n",
    "    },\n",
    "    \"arch\": {\n",
    "        \"type\": \"SentenceTransformersWrapperForLM\",\n",
    "        \"args\": {\n",
    "            \"model_name\": \"all-MiniLM-L6-v2\",\n",
    "            \"model_path\": None,\n",
    "            \"hidden_size\": 512,\n",
    "            \"dropout\": 0.1,\n",
    "            \"vocab_size\": 32000,\n",
    "            \"load_path\": 'E:\\\\OneDrive - Hanoi University of Science and Technology\\\\Chuyen nganh\\\\Deep Learning and Its Applications\\\\BTL\\\\DeepLearning20231\\\\saved\\\\models\\\\sentence_bert\\\\1211_113344\\\\checkpoint-epoch1.pth',\n",
    "        }\n",
    "    },\n",
    "    \"data_loader\": {\n",
    "        \"type\": \"MLMDataLoader\",\n",
    "        \"args\":{\n",
    "            \"data_path\": \"./first_1024_paras.tsv\",\n",
    "            \"batch_size\": 4,\n",
    "            \"shuffle\": True,\n",
    "            \"validation_split\": 0.1,\n",
    "            \"num_workers\": 2\n",
    "        }\n",
    "    },\n",
    "    \"optimizer\": {\n",
    "        \"type\": \"AdamW\",\n",
    "        \"args\":{\n",
    "            \"lr\": 0.001,\n",
    "            \"weight_decay\": 0,\n",
    "            \"amsgrad\": True\n",
    "        }\n",
    "    },\n",
    "    \"loss\": \"mlm_loss\",\n",
    "    \"metrics\": [\n",
    "        \"mlm_accuracy\"\n",
    "    ],\n",
    "    \"lr_scheduler\": {\n",
    "        \"type\": \"StepLR\",\n",
    "        \"args\": {\n",
    "            \"step_size\": 1,\n",
    "            \"gamma\": 0.9,\n",
    "        }\n",
    "    },\n",
    "    \"trainer\": {\n",
    "        \"epochs\": 5,\n",
    "\n",
    "        \"save_dir\": \"saved/\",\n",
    "        \"save_period\": 1,\n",
    "        \"verbosity\": 2,\n",
    "        \n",
    "        \"monitor\": \"min val_loss\",\n",
    "        \"early_stop\": 2,\n",
    "\n",
    "        \"tensorboard\": True\n",
    "    }\n",
    "}\n",
    "\n",
    "config = ConfigParser(config)\n",
    "logger = config.get_logger('train')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Load pretrained SentenceTransformer: all-MiniLM-L6-v2\n",
      "Use pytorch device: cuda\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "e:\\anaconda3\\envs\\dl-env\\lib\\site-packages\\torch\\nn\\modules\\lazy.py:180: UserWarning: Lazy modules are a new feature under heavy development so changes to the API or functionality can happen at any moment.\n",
      "  warnings.warn('Lazy modules are a new feature under heavy development '\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loaded model from E:\\OneDrive - Hanoi University of Science and Technology\\Chuyen nganh\\Deep Learning and Its Applications\\BTL\\DeepLearning20231\\saved\\models\\sentence_bert\\1211_113344\\checkpoint-epoch1.pth\n",
      "SentenceTransformersWrapperForLM(\n",
      "  (model): SentenceTransformer(\n",
      "    (0): Transformer({'max_seq_length': 256, 'do_lower_case': False}) with Transformer model: BertModel \n",
      "    (1): Pooling({'word_embedding_dimension': 384, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False})\n",
      "    (2): Normalize()\n",
      "  )\n",
      "  (lm_output_layer): Sequential(\n",
      "    (0): LazyLinear(in_features=0, out_features=512, bias=True)\n",
      "    (1): Dropout(p=0.1, inplace=False)\n",
      "    (2): Linear(in_features=512, out_features=32000, bias=True)\n",
      "  )\n",
      ")\n",
      "Trainable parameters: 39893888\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5fec681ff5ba43a49a5ffa65fd04f127",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1023 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "82b68855f35a463eb77db63d82e1cbb4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/1023 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "fdee90945eb2493a96e4ea870c2972c7",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Map:   0%|          | 0/219 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You're using a BertTokenizerFast tokenizer. Please note that with a fast tokenizer, using the `__call__` method is faster than using a method to encode the text followed by a call to the `pad` method to get a padded encoding.\n"
     ]
    }
   ],
   "source": [
    "# get tokenizer, model and print model architecture\n",
    "tokenizer = AutoTokenizer.from_pretrained(config['tokenizer']['path'])\n",
    "model = config.init_obj('arch', module_arch)\n",
    "logger.info(model)\n",
    "\n",
    "# setup data_loader instances\n",
    "data_loader = config.init_obj('data_loader', module_data, tokenizer)\n",
    "valid_data_loader = data_loader.split_validation()\n",
    "\n",
    "# prepare for (multi-device) GPU training\n",
    "device, device_ids = prepare_device(config['n_gpu'])\n",
    "model = model.to(device)\n",
    "if len(device_ids) > 1:\n",
    "    model = torch.nn.DataParallel(model, device_ids=device_ids)\n",
    "\n",
    "# get function handles of loss and metrics\n",
    "criterion = getattr(module_loss, config['loss'])\n",
    "metrics = [getattr(module_metric, met) for met in config['metrics']]\n",
    "\n",
    "# build optimizer, learning rate scheduler. delete every lines containing lr_scheduler for disabling scheduler\n",
    "trainable_params = filter(lambda p: p.requires_grad, model.parameters())\n",
    "optimizer = config.init_obj('optimizer', torch.optim, trainable_params)\n",
    "lr_scheduler = config.init_obj('lr_scheduler', torch.optim.lr_scheduler, optimizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainer = BertTrainer(model, criterion, metrics, optimizer,\n",
    "                    config=config,\n",
    "                    device=device,\n",
    "                    data_loader=data_loader,\n",
    "                    valid_data_loader=valid_data_loader,\n",
    "                    lr_scheduler=lr_scheduler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train Epoch: 1 [0/198 (0%)] Loss: 2.162666\n",
      "Train Epoch: 1 [8/198 (4%)] Loss: 5.435498\n",
      "Train Epoch: 1 [16/198 (8%)] Loss: 4.857285\n",
      "Train Epoch: 1 [24/198 (12%)] Loss: 4.854122\n",
      "Train Epoch: 1 [32/198 (16%)] Loss: 4.575061\n",
      "Train Epoch: 1 [40/198 (20%)] Loss: 4.442165\n",
      "Train Epoch: 1 [48/198 (24%)] Loss: 4.727591\n",
      "Train Epoch: 1 [56/198 (28%)] Loss: 4.426732\n",
      "Train Epoch: 1 [64/198 (32%)] Loss: 4.536580\n",
      "Train Epoch: 1 [72/198 (36%)] Loss: 4.848359\n",
      "Train Epoch: 1 [80/198 (40%)] Loss: 4.954525\n",
      "Train Epoch: 1 [88/198 (44%)] Loss: 4.827743\n",
      "Train Epoch: 1 [96/198 (48%)] Loss: 4.237995\n",
      "Train Epoch: 1 [104/198 (53%)] Loss: 4.580791\n",
      "Train Epoch: 1 [112/198 (57%)] Loss: 4.661310\n",
      "Train Epoch: 1 [120/198 (61%)] Loss: 4.529194\n",
      "Train Epoch: 1 [128/198 (65%)] Loss: 4.486995\n",
      "Train Epoch: 1 [136/198 (69%)] Loss: 4.289148\n",
      "Train Epoch: 1 [144/198 (73%)] Loss: 4.455792\n",
      "Train Epoch: 1 [152/198 (77%)] Loss: 4.384675\n",
      "Train Epoch: 1 [160/198 (81%)] Loss: 4.927923\n",
      "Train Epoch: 1 [168/198 (85%)] Loss: 3.862172\n",
      "Train Epoch: 1 [176/198 (89%)] Loss: 4.464448\n",
      "Train Epoch: 1 [184/198 (93%)] Loss: 4.448378\n",
      "Train Epoch: 1 [192/198 (97%)] Loss: 4.655560\n",
      "    epoch          : 1\n",
      "    loss           : 4.581803512573242\n",
      "    mlm_accuracy   : 0.26564362770548444\n",
      "    val_loss       : 4.240772445996602\n",
      "    val_mlm_accuracy: 0.3030558510780352\n",
      "Saving checkpoint: saved\\models\\sentence_bert\\1211_214644\\checkpoint-epoch1.pth ...\n",
      "Saving current best: model_best.pth ...\n",
      "Train Epoch: 2 [0/198 (0%)] Loss: 2.797075\n",
      "Train Epoch: 2 [8/198 (4%)] Loss: 3.001528\n",
      "Train Epoch: 2 [16/198 (8%)] Loss: 3.757712\n",
      "Train Epoch: 2 [24/198 (12%)] Loss: 2.858841\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[6], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32me:\\OneDrive - Hanoi University of Science and Technology\\Chuyen nganh\\Deep Learning and Its Applications\\BTL\\DeepLearning20231\\base\\base_trainer.py:63\u001b[0m, in \u001b[0;36mBaseTrainer.train\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     61\u001b[0m not_improved_count \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[0;32m     62\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m epoch \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstart_epoch, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mepochs \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m):\n\u001b[1;32m---> 63\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_train_epoch\u001b[49m\u001b[43m(\u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     65\u001b[0m     \u001b[38;5;66;03m# save logged informations into log dict\u001b[39;00m\n\u001b[0;32m     66\u001b[0m     log \u001b[38;5;241m=\u001b[39m {\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mepoch\u001b[39m\u001b[38;5;124m'\u001b[39m: epoch}\n",
      "File \u001b[1;32me:\\OneDrive - Hanoi University of Science and Technology\\Chuyen nganh\\Deep Learning and Its Applications\\BTL\\DeepLearning20231\\trainer\\trainer.py:136\u001b[0m, in \u001b[0;36mBertTrainer._train_epoch\u001b[1;34m(self, epoch)\u001b[0m\n\u001b[0;32m    133\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39moptimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m    135\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwriter\u001b[38;5;241m.\u001b[39mset_step((epoch \u001b[38;5;241m-\u001b[39m \u001b[38;5;241m1\u001b[39m) \u001b[38;5;241m*\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlen_epoch \u001b[38;5;241m+\u001b[39m batch_idx)\n\u001b[1;32m--> 136\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain_metrics\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mupdate\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mloss\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mitem\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    137\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m met \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetric_ftns:\n\u001b[0;32m    138\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrain_metrics\u001b[38;5;241m.\u001b[39mupdate(met\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m, met(output, target))\n",
      "File \u001b[1;32me:\\OneDrive - Hanoi University of Science and Technology\\Chuyen nganh\\Deep Learning and Its Applications\\BTL\\DeepLearning20231\\utils\\util.py:56\u001b[0m, in \u001b[0;36mMetricTracker.update\u001b[1;34m(self, key, value, n)\u001b[0m\n\u001b[0;32m     53\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m col \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data\u001b[38;5;241m.\u001b[39mcolumns:\n\u001b[0;32m     54\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_data[col]\u001b[38;5;241m.\u001b[39mvalues[:] \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m0\u001b[39m\n\u001b[1;32m---> 56\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mupdate\u001b[39m(\u001b[38;5;28mself\u001b[39m, key, value, n\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1\u001b[39m):\n\u001b[0;32m     57\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwriter \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m     58\u001b[0m         \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mwriter\u001b[38;5;241m.\u001b[39madd_scalar(key, value)\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "trainer.train()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dl-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
