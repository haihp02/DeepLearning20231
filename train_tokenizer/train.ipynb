{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\anaconda3\\envs\\thanhnx\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "from transformers import AutoTokenizer\n",
    "tokenizer = AutoTokenizer.from_pretrained(\"sentence-transformers/all-MiniLM-L6-v2\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tokenizers.models.WordPiece"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# check tokenizer type\n",
    "tokenizer.backend_tokenizer.model.__class__"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Class definition for VocabAugmentor.\"\"\"\n",
    "from collections import Counter\n",
    "from pathlib import Path\n",
    "from tempfile import NamedTemporaryFile, TemporaryDirectory\n",
    "from types import MappingProxyType\n",
    "from typing import IO\n",
    "from typing import Counter as CounterType\n",
    "from typing import List, Type, Union\n",
    "\n",
    "from sklearn.base import BaseEstimator\n",
    "from tokenizers import Tokenizer, trainers\n",
    "from tokenizers.implementations import BaseTokenizer\n",
    "from tokenizers.models import BPE, Unigram, WordPiece\n",
    "from tokenizers.normalizers import Lowercase\n",
    "from tokenizers.pre_tokenizers import Whitespace\n",
    "from transformers import PreTrainedTokenizerFast\n",
    "\n",
    "#from transformers_domain_adaptation.type import Corpus, Token\n",
    "from typing import NewType, Sequence\n",
    "\n",
    "Token = NewType(\"Token\", str)\n",
    "Document = NewType(\"Document\", str)\n",
    "Corpus = NewType(\"Corpus\", Sequence[Document])\n",
    "\n",
    "class VocabAugmentor(BaseEstimator):\n",
    "    \"\"\"Find new tokens to add to a :obj:`tokenizer`'s vocabulary.\n",
    "\n",
    "    A new vocabulary is learnt from the training corpus\n",
    "    using the same tokenization model (WordPiece, BPE, Unigram).\n",
    "    The most common tokens of this new vocabulary that do not exist\n",
    "    in the existing vocabulary are selected.\n",
    "    \"\"\"\n",
    "\n",
    "    supported_trainers = MappingProxyType(\n",
    "        {\n",
    "            BPE: trainers.BpeTrainer,\n",
    "            WordPiece: trainers.WordPieceTrainer,\n",
    "            Unigram: trainers.UnigramTrainer,\n",
    "        }\n",
    "    )\n",
    "\n",
    "    def __init__(\n",
    "        self, tokenizer: PreTrainedTokenizerFast, cased: bool, target_vocab_size: int\n",
    "    ):\n",
    "        \"\"\"\n",
    "        Args:\n",
    "            tokenizer: A Rust-based ðŸ¤— Tokenizer\n",
    "            cased: If False, ignore uppercases in corpus\n",
    "            target_vocab_size: Size of augmented vocabulary\n",
    "\n",
    "        Raises:\n",
    "            ValueError: If :obj:`target_vocab_size` is larger or equal to the existing vocabulary of :obj:`tokenizer`\n",
    "            RuntimeError: If :obj:`tokenizer` uses an unsupported tokenization model\n",
    "        \"\"\"\n",
    "        if target_vocab_size <= tokenizer.vocab_size:\n",
    "            raise ValueError(\n",
    "                f\"Ensure that `target_vocab_size` is larger than tokenizer's vocab size.\"\n",
    "            )\n",
    "        self.tokenizer = tokenizer\n",
    "        self.cased = cased\n",
    "        self.target_vocab_size = target_vocab_size\n",
    "        self.model_cls: Type[\n",
    "            BaseTokenizer\n",
    "        ] = tokenizer.backend_tokenizer.model.__class__\n",
    "\n",
    "        # Instantiate rust tokenizer\n",
    "        rust_tokenizer = Tokenizer(self.model_cls())\n",
    "        if not cased:\n",
    "            rust_tokenizer.normalizer = Lowercase()\n",
    "        rust_tokenizer.pre_tokenizer = Whitespace()\n",
    "        self.rust_tokenizer = rust_tokenizer\n",
    "\n",
    "        # Instantiate the appropriate Trainer based on `self.model` (i.e. BPE, WordPiece, etc)\n",
    "        trainer_cls = self.supported_trainers.get(self.model_cls, None)\n",
    "        if trainer_cls is None:\n",
    "            raise RuntimeError(f\"{self.model_cls} is not supported\")\n",
    "        self.trainer = trainer_cls(\n",
    "            vocab_size=self.target_vocab_size,\n",
    "            special_tokens=list(self.tokenizer.special_tokens_map.values()),\n",
    "        )\n",
    "\n",
    "    def get_new_tokens(\n",
    "        self,\n",
    "        training_corpus: Union[Corpus, Path, str],\n",
    "    ) -> List[Token]:\n",
    "        \"\"\"Obtain new tokens found in :obj:`training_corpus`.\n",
    "\n",
    "        New tokens contains the most common tokens that do not exist in the :obj:`tokenizer`'s vocabulary.\n",
    "\n",
    "        Args:\n",
    "            training_corpus: The training corpus\n",
    "        \"\"\"\n",
    "        # Training has to be wrapped with the `tmpfile` context\n",
    "        with NamedTemporaryFile(\"w+\") as tmpfile:  # If we need to save Corpus type\n",
    "            # Train new tokenizer on `ft_corpus`\n",
    "            train_files = self._get_training_files(training_corpus, _tmpfile=tmpfile)\n",
    "            self.rust_tokenizer.train(train_files, self.trainer)\n",
    "\n",
    "            # Include unknown token to vocab\n",
    "            with TemporaryDirectory() as tmpdir:\n",
    "                files = self.rust_tokenizer.model.save(tmpdir)\n",
    "                self.rust_tokenizer.model = self.model_cls.from_file(\n",
    "                    *files, unk_token=\"[UNK]\"\n",
    "                )\n",
    "\n",
    "            # Find most common tokens in vocab\n",
    "            token_counts = self._count_tokens(train_files)\n",
    "\n",
    "        # Remove overlapping tokens from original tokenizer\n",
    "        token_counts = self._remove_overlapping_tokens(token_counts)\n",
    "        new_tokens = [\n",
    "            token\n",
    "            for token, _ in token_counts.most_common(\n",
    "                self.target_vocab_size - self.tokenizer.vocab_size\n",
    "            )\n",
    "        ]\n",
    "        return new_tokens\n",
    "\n",
    "    @staticmethod\n",
    "    def _get_training_files(\n",
    "        corpus: Union[Corpus, Path, str], _tmpfile: IO[str]\n",
    "    ) -> List[str]:\n",
    "        \"\"\"Return files for training.\n",
    "\n",
    "        If `corpus is a sequence of documents, it will be written to a temporary file,\n",
    "        and that temporary file's name will be returned.\n",
    "\n",
    "        If `corpus` is a Path or str, it will return the path, or paths if `corpus` is a directory.\n",
    "\n",
    "        Args:\n",
    "            corpus: Text data or path to training corpus\n",
    "            _tmpfile: Temporary file object. Used when `corpus` is not a path\n",
    "\n",
    "        Raises:\n",
    "            FileNotFoundError: If `corpus` is a str or Path and it does not exist on the filesystem.\n",
    "        \"\"\"\n",
    "        if isinstance(corpus, str) or isinstance(corpus, Path):\n",
    "            corpus = Path(corpus)\n",
    "\n",
    "            if not corpus.exists():\n",
    "                raise FileNotFoundError(\n",
    "                    f\"Training corpus {corpus.as_posix()} does not exist.\"\n",
    "                )\n",
    "\n",
    "            files = list(corpus.rglob(\"*.*\")) if corpus.is_dir() else [corpus]\n",
    "            files = [f.as_posix() for f in files]\n",
    "            return files\n",
    "\n",
    "        else:  # Corpus type\n",
    "            for doc in corpus:\n",
    "                _tmpfile.write(doc)\n",
    "            _tmpfile.seek(0)\n",
    "            return [_tmpfile.name]\n",
    "\n",
    "    def _count_tokens(self, files: List[str]) -> CounterType[str]:\n",
    "        \"\"\"Count number of tokens in a list of files.\"\"\"\n",
    "        token_counts: CounterType[str] = Counter()\n",
    "        for file in files:\n",
    "            with open(file) as f:\n",
    "                token_counts += Counter(\n",
    "                    token\n",
    "                    for enc in self.rust_tokenizer.encode_batch(f.readlines())\n",
    "                    for token in enc.tokens\n",
    "                )\n",
    "        return token_counts\n",
    "\n",
    "    def _remove_overlapping_tokens(\n",
    "        self, token_counts: CounterType[str]\n",
    "    ) -> CounterType[str]:\n",
    "        \"\"\"Remove tokens from `token_counts` that exist in the current tokenizer's vocab.\"\"\"\n",
    "        _token_counts = token_counts.copy()\n",
    "\n",
    "        for vocab_term in self.tokenizer.get_vocab().keys():\n",
    "            if vocab_term in _token_counts:\n",
    "                del _token_counts[vocab_term]\n",
    "        return _token_counts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "target_vocab_size = 32000  # len(tokenizer) == 30_522\n",
    "\n",
    "augmentor = VocabAugmentor(\n",
    "    tokenizer=tokenizer,\n",
    "    cased=False,\n",
    "    target_vocab_size=target_vocab_size\n",
    ")\n",
    "\n",
    "# Obtain new domain-specific terminology based on the fine-tuning corpus\n",
    "#new_tokens: List[str] = augmentor.get_new_tokens(['./data/raw_texts/collection_corpus.txt' , './data/raw_texts/dev_corpus.txt', './data/raw_texts/eval_corpus.txt' , './data/raw_texts/train_copus.txt'])\n",
    "new_tokens: List[str] = augmentor.get_new_tokens('./data/raw_texts/')\n",
    "\n",
    "assert len(new_tokens) == (target_vocab_size - len(tokenizer))\n",
    "\n",
    "# Update ``model`` and ``tokenizer`` with these newfound terms\n",
    "tokenizer.add_tokens(new_tokens)\n",
    "#model.resize_token_embeddings(len(tokenizer))\n",
    "\n",
    "assert len(tokenizer) == target_vocab_size\n",
    "#assert model.get_input_embeddings().shape[1] == target_vocab_size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "thanhnx",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
